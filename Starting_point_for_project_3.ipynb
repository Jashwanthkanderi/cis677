{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOUJ46gFQtB5WJCOwIBrO0C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/trefftzc/cis677/blob/main/Starting_point_for_project_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Starting point for project 3\n",
        "\n",
        "For project 3, you will write a parallel version of the softmax function that will work on a GPU. The suggested environment is numba/cuda.\n",
        "\n",
        "\n",
        "\n",
        "You can find more information about the softmax function in\n",
        " [Wikipedia](https://en.wikipedia.org/wiki/Softmax_function)\n",
        "\n",
        " The code below is a sequential, step by step implementation in python of the softmax function."
      ],
      "metadata": {
        "id": "SE0L4pksGB7p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12Jph5X5F-89",
        "outputId": "7b8ce421-0609-4cca-9cad-1929deb62046"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The original array:  [1. 2. 3. 4.]\n",
            "The array after calling expon:  [ 2.71828183  7.3890561  20.08553692 54.59815003]\n",
            "The sum is:  84.7910248837216\n",
            "The result:  [0.0320586  0.08714432 0.23688282 0.64391426]\n",
            "The sum of the values in result is:  1.0\n"
          ]
        }
      ],
      "source": [
        "# Softmax: An important function for ML\n",
        "import numpy as np\n",
        "\n",
        "# Return e**value\n",
        "def expon(value):\n",
        "    return np.exp(value)\n",
        "\n",
        "# Return value/sum\n",
        "def normalize(value,sum):\n",
        "    return value/sum\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    a = np.zeros(4,dtype=float)\n",
        "    for i in range(4):\n",
        "        a[i] = (i+1)*1.0\n",
        "\n",
        "    print(\"The original array: \",a)\n",
        "\n",
        "    b = np.zeros(4,dtype=float)\n",
        "    for i in range(4):\n",
        "        b[i] = expon(a[i])\n",
        "\n",
        "    print(\"The array after calling expon: \",b)\n",
        "\n",
        "    sum_values = np.sum(b)\n",
        "    print(\"The sum is: \",sum_values)\n",
        "\n",
        "    result = np.zeros(4,dtype=float)\n",
        "    for i in range(4):\n",
        "        result[i] = normalize(b[i],sum_values)\n",
        "\n",
        "    print(\"The result: \",result)\n",
        "    print(\"The sum of the values in result is: \",np.sum(result))"
      ]
    }
  ]
}